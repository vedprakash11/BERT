{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](data:image/png;base64,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)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. What is BERT?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**BERT is a deep learning model that has given state-of-the-art results on a wide variety of natural language processing tasks. It stands for Bidirectional Encoder Representations for Transformers. It has been pre-trained on Wikipedia and BooksCorpus and requires task-specific fine-tuning**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. Why BERT?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* BERT was built upon recent work and clever ideas in pre-training contextual representations including Semi-supervised Sequence Learning, Generative Pre-Training, ELMo, the OpenAI Transformer, ULMFit and the Transformer. Although these models are all unidirectional or shallowly bidirectional, BERT is fully bidirectional.\n",
    "* BERT gives it incredible accuracy and performance on smaller data sets which solves a huge problem in natural language processing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. How does it work?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BERT relies on a Transformer (the attention mechanism that learns contextual relationships between words in a text). A basic Transformer consists of an encoder to read the text input and a decoder to produce a prediction for the task. Since BERTâ€™s goal is to generate a language representation model, it only needs the encoder part. The input to the encoder for BERT is a sequence of tokens, which are first converted into vectors and then processed in the neural network. But before processing can start, BERT needs the input to be massaged and decorated with some extra metadata:\n",
    "\n",
    "1. Token embeddings: A [CLS] token is added to the input word tokens at the beginning of the first sentence and a [SEP] token is inserted at the end of each sentence.\n",
    "2. Segment embeddings: A marker indicating Sentence A or Sentence B is added to each token. This allows the encoder to distinguish between sentences.\n",
    "3. Positional embeddings: A positional embedding is added to each token to indicate its position in the sentence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4. How to use BERT?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**BERT can be used for a wide variety of language tasks, while only adding a small layer to the core model**\n",
    "\n",
    "1. Classification tasks such as sentiment analysis are done similarly to Next Sentence classification, by adding a classification layer on top of the Transformer output for the [CLS] token.\n",
    "\n",
    "2. In Question Answering tasks (e.g. SQuAD v1.1), the software receives a question regarding a text sequence and is required to mark the answer in the sequence. Using BERT, a Q&A model can be trained by learning two extra vectors that mark the beginning and the end of the answer.\n",
    "\n",
    "3. In Named Entity Recognition (NER), the software receives a text sequence and is required to mark the various types of entities (Person, Organization, Date, etc) that appear in the text. Using BERT, a NER model can be trained by feeding the output vector of each token into a classification layer that predicts the NER label."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Import Libraries and Data** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'wget' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    }
   ],
   "source": [
    "# import BERT tokenization\n",
    "\n",
    "!wget --quiet https://raw.githubusercontent.com/tensorflow/models/master/official/nlp/bert/tokenization.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bert import tokenization\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv('Corona_NLP_train.csv', encoding='latin-1')\n",
    "test_data = pd.read_csv('Corona_NLP_test.csv', encoding='latin-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>UserName</th>\n",
       "      <th>ScreenName</th>\n",
       "      <th>Location</th>\n",
       "      <th>TweetAt</th>\n",
       "      <th>OriginalTweet</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3799</td>\n",
       "      <td>48751</td>\n",
       "      <td>London</td>\n",
       "      <td>16-03-2020</td>\n",
       "      <td>@MeNyrbie @Phil_Gahan @Chrisitv https://t.co/i...</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3800</td>\n",
       "      <td>48752</td>\n",
       "      <td>UK</td>\n",
       "      <td>16-03-2020</td>\n",
       "      <td>advice Talk to your neighbours family to excha...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3801</td>\n",
       "      <td>48753</td>\n",
       "      <td>Vagabonds</td>\n",
       "      <td>16-03-2020</td>\n",
       "      <td>Coronavirus Australia: Woolworths to give elde...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3802</td>\n",
       "      <td>48754</td>\n",
       "      <td>NaN</td>\n",
       "      <td>16-03-2020</td>\n",
       "      <td>My food stock is not the only one which is emp...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3803</td>\n",
       "      <td>48755</td>\n",
       "      <td>NaN</td>\n",
       "      <td>16-03-2020</td>\n",
       "      <td>Me, ready to go at supermarket during the #COV...</td>\n",
       "      <td>Extremely Negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   UserName  ScreenName   Location     TweetAt  \\\n",
       "0      3799       48751     London  16-03-2020   \n",
       "1      3800       48752         UK  16-03-2020   \n",
       "2      3801       48753  Vagabonds  16-03-2020   \n",
       "3      3802       48754        NaN  16-03-2020   \n",
       "4      3803       48755        NaN  16-03-2020   \n",
       "\n",
       "                                       OriginalTweet           Sentiment  \n",
       "0  @MeNyrbie @Phil_Gahan @Chrisitv https://t.co/i...             Neutral  \n",
       "1  advice Talk to your neighbours family to excha...            Positive  \n",
       "2  Coronavirus Australia: Woolworths to give elde...            Positive  \n",
       "3  My food stock is not the only one which is emp...            Positive  \n",
       "4  Me, ready to go at supermarket during the #COV...  Extremely Negative  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>UserName</th>\n",
       "      <th>ScreenName</th>\n",
       "      <th>Location</th>\n",
       "      <th>TweetAt</th>\n",
       "      <th>OriginalTweet</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>44953</td>\n",
       "      <td>NYC</td>\n",
       "      <td>02-03-2020</td>\n",
       "      <td>TRENDING: New Yorkers encounter empty supermar...</td>\n",
       "      <td>Extremely Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>44954</td>\n",
       "      <td>Seattle, WA</td>\n",
       "      <td>02-03-2020</td>\n",
       "      <td>When I couldn't find hand sanitizer at Fred Me...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>44955</td>\n",
       "      <td>NaN</td>\n",
       "      <td>02-03-2020</td>\n",
       "      <td>Find out how you can protect yourself and love...</td>\n",
       "      <td>Extremely Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>44956</td>\n",
       "      <td>Chicagoland</td>\n",
       "      <td>02-03-2020</td>\n",
       "      <td>#Panic buying hits #NewYork City as anxious sh...</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>44957</td>\n",
       "      <td>Melbourne, Victoria</td>\n",
       "      <td>03-03-2020</td>\n",
       "      <td>#toiletpaper #dunnypaper #coronavirus #coronav...</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   UserName  ScreenName             Location     TweetAt  \\\n",
       "0         1       44953                  NYC  02-03-2020   \n",
       "1         2       44954          Seattle, WA  02-03-2020   \n",
       "2         3       44955                  NaN  02-03-2020   \n",
       "3         4       44956          Chicagoland  02-03-2020   \n",
       "4         5       44957  Melbourne, Victoria  03-03-2020   \n",
       "\n",
       "                                       OriginalTweet           Sentiment  \n",
       "0  TRENDING: New Yorkers encounter empty supermar...  Extremely Negative  \n",
       "1  When I couldn't find hand sanitizer at Fred Me...            Positive  \n",
       "2  Find out how you can protect yourself and love...  Extremely Positive  \n",
       "3  #Panic buying hits #NewYork City as anxious sh...            Negative  \n",
       "4  #toiletpaper #dunnypaper #coronavirus #coronav...             Neutral  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Label encoding of labels**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [1. 0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "label = preprocessing.LabelEncoder()\n",
    "y = label.fit_transform(train_data['Sentiment'])\n",
    "y = to_categorical(y)\n",
    "print(y[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Build a BERT layer**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we create a BERT embedding layer by importing the BERT model from hub.KerasLayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_url = 'https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/2'\n",
    "bert_layer = hub.KerasLayer(m_url, trainable=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Encoding the text**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we create a BERT vocab_file in the form a numpy array. We then set the text to lowercase and finally we pass our vocab_file and do_lower_case variables to the Tokenizer object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'tensorflow' has no attribute 'gfile'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-17-a64ed504e74e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mvocab_file\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbert_layer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresolved_object\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvocab_file\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masset_path\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mdo_lower_case\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbert_layer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresolved_object\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdo_lower_case\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mtokenizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtokenization\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mFullTokenizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvocab_file\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdo_lower_case\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mbert_encode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtexts\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_len\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m512\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\Developer\\lib\\site-packages\\bert\\tokenization.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, vocab_file, do_lower_case)\u001b[0m\n\u001b[0;32m    163\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    164\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvocab_file\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdo_lower_case\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 165\u001b[1;33m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvocab\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload_vocab\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvocab_file\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    166\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minv_vocab\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[0mv\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mk\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mv\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    167\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbasic_tokenizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mBasicTokenizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdo_lower_case\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdo_lower_case\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\Developer\\lib\\site-packages\\bert\\tokenization.py\u001b[0m in \u001b[0;36mload_vocab\u001b[1;34m(vocab_file)\u001b[0m\n\u001b[0;32m    123\u001b[0m   \u001b[0mvocab\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcollections\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOrderedDict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    124\u001b[0m   \u001b[0mindex\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 125\u001b[1;33m   \u001b[1;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mio\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgfile\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mGFile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvocab_file\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"r\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mreader\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    126\u001b[0m     \u001b[1;32mwhile\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    127\u001b[0m       \u001b[0mtoken\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconvert_to_unicode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'tensorflow' has no attribute 'gfile'"
     ]
    }
   ],
   "source": [
    "vocab_file = bert_layer.resolved_object.vocab_file.asset_path.numpy()\n",
    "do_lower_case = bert_layer.resolved_object.do_lower_case.numpy()\n",
    "tokenizer = tokenization.FullTokenizer(vocab_file, do_lower_case)\n",
    "\n",
    "def bert_encode(texts, tokenizer, max_len=512):\n",
    "    all_tokens = []\n",
    "    all_masks = []\n",
    "    all_segments = []\n",
    "    \n",
    "    for text in texts:\n",
    "        text = tokenizer.tokenize(text)\n",
    "        \n",
    "        text = text[:max_len-2]\n",
    "        input_sequence = [\"[CLS]\"] + text + [\"[SEP]\"]\n",
    "        pad_len = max_len-len(input_sequence)\n",
    "        \n",
    "        tokens = tokenizer.convert_tokens_to_ids(input_sequence) + [0] * pad_len\n",
    "        pad_masks = [1] * len(input_sequence) + [0] * pad_len\n",
    "        segment_ids = [0] * max_len\n",
    "        \n",
    "        all_tokens.append(tokens)\n",
    "        all_masks.append(pad_masks)\n",
    "        all_segments.append(segment_ids)\n",
    "        \n",
    "    return np.array(all_tokens), np.array(all_masks), np.array(all_segments)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Build The Model**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are all set to create our model. To do so, we will create a function named build_model that having tf.keras.models.Model class. Inside the function we will define our model layers. Our model will consist of three **Dense** neural network layers and also dropout layer. We have chosen a learning rate to 1e-5.\n",
    "\n",
    "**RELU function** :- \n",
    "With default values, this returns max(x, 0), the element-wise maximum of 0 and the input tensor.\n",
    "Modifying default parameters allows you to use non-zero thresholds, change the max value of the activation, and to use a non-zero multiple of the input for values below the threshold.\n",
    "\n",
    "\n",
    "**Softmax function** :-\n",
    "Softmax converts a real vector to a vector of categorical probabilities.\n",
    "The elements of the output vector are in range (0, 1) and sum to 1.\n",
    "Each vector is handled independently. The axis argument sets which axis of the input the function is applied along.\n",
    "Softmax is often used as the activation for the last layer of a classification network because the result could be interpreted as a probability distribution.\n",
    "The softmax of each vector x is computed as exp(x) / tf.reduce_sum(exp(x)).\n",
    "\n",
    "**Binary corssentropy**:-\n",
    "Computes the cross-entropy loss between true labels and predicted labels.\n",
    "We can use this cross-entropy loss when there are only two label classes (assumed to be 0 and 1). For each example, there should be a single floating-point value per prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(bert_layer, max_len=512):\n",
    "    input_word_ids = tf.keras.Input(shape=(max_len,), dtype=tf.int32, name=\"input_word_ids\")\n",
    "    input_mask = tf.keras.Input(shape=(max_len,), dtype=tf.int32, name=\"input_mask\")\n",
    "    segment_ids = tf.keras.Input(shape=(max_len,), dtype=tf.int32, name=\"segment_ids\")\n",
    "    \n",
    "    pooled_output, sequence_output = bert_layer([input_word_ids, input_mask, segment_ids])\n",
    "    \n",
    "    clf_output = sequence_output[:, 0, :]\n",
    "    \n",
    "    lay = tf.keras.layers.Dense(64, activation='relu')(clf_output)\n",
    "    lay = tf.keras.layers.Dropout(0.2)(lay)\n",
    "    lay = tf.keras.layers.Dense(32, activation='relu')(lay)\n",
    "    lay = tf.keras.layers.Dropout(0.2)(lay)\n",
    "    out = tf.keras.layers.Dense(5, activation='softmax')(lay)\n",
    "    \n",
    "    model = tf.keras.models.Model(inputs=[input_word_ids, input_mask, segment_ids], outputs=out)\n",
    "    model.compile(tf.keras.optimizers.Adam(lr=2e-5), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here We check only the first 250 characters of each text, and also we set train-test input and train labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = 250\n",
    "train_input = bert_encode(train_data.OriginalTweet.values, tokenizer, max_len=max_len)\n",
    "test_input = bert_encode(test_data.OriginalTweet.values, tokenizer, max_len=max_len)\n",
    "train_labels = y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Extremely Negative' 'Extremely Positive' 'Negative' 'Neutral' 'Positive']\n"
     ]
    }
   ],
   "source": [
    "labels = label.classes_\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_word_ids (InputLayer)     [(None, 250)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_mask (InputLayer)         [(None, 250)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "segment_ids (InputLayer)        [(None, 250)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "keras_layer (KerasLayer)        [(None, 768), (None, 109482241   input_word_ids[0][0]             \n",
      "                                                                 input_mask[0][0]                 \n",
      "                                                                 segment_ids[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice (Tens [(None, 768)]        0           keras_layer[0][1]                \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 64)           49216       tf_op_layer_strided_slice[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "dropout (Dropout)               (None, 64)           0           dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 32)           2080        dropout[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 32)           0           dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 5)            165         dropout_1[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 109,533,702\n",
      "Trainable params: 109,533,701\n",
      "Non-trainable params: 1\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = build_model(bert_layer, max_len=max_len)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Run the model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "1029/1029 [==============================] - ETA: 0s - loss: 1.1575 - accuracy: 0.5183\n",
      "Epoch 00001: val_accuracy improved from -inf to 0.79470, saving model to model.h5\n",
      "1029/1029 [==============================] - 1057s 1s/step - loss: 1.1575 - accuracy: 0.5183 - val_loss: 0.5773 - val_accuracy: 0.7947\n",
      "Epoch 2/3\n",
      "1029/1029 [==============================] - ETA: 0s - loss: 0.6357 - accuracy: 0.7884\n",
      "Epoch 00002: val_accuracy improved from 0.79470 to 0.82908, saving model to model.h5\n",
      "1029/1029 [==============================] - 1057s 1s/step - loss: 0.6357 - accuracy: 0.7884 - val_loss: 0.4788 - val_accuracy: 0.8291\n",
      "Epoch 3/3\n",
      "1029/1029 [==============================] - ETA: 0s - loss: 0.4301 - accuracy: 0.8624\n",
      "Epoch 00003: val_accuracy improved from 0.82908 to 0.86334, saving model to model.h5\n",
      "1029/1029 [==============================] - 1058s 1s/step - loss: 0.4301 - accuracy: 0.8624 - val_loss: 0.4275 - val_accuracy: 0.8633\n"
     ]
    }
   ],
   "source": [
    "checkpoint = tf.keras.callbacks.ModelCheckpoint('model.h5', monitor='val_accuracy', save_best_only=True, verbose=1)\n",
    "earlystopping = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=5, verbose=1)\n",
    "\n",
    "train_sh = model.fit(\n",
    "    train_input, train_labels,\n",
    "    validation_split=0.2,\n",
    "    epochs=3,\n",
    "    callbacks=[checkpoint, earlystopping],\n",
    "    batch_size=32,\n",
    "    verbose=1\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
